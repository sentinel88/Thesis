\chapter{Evaluation}\label{chapter:evaluation}
The evaluation of job schedulers depend on two important factors: use of appropriate metrics, and the use of appropriate workloads on which the scheduler can operate. Before we consider these two factors in details, the following definitions will be assumed going forward.
\begin{itemize}
\item \textit{${t_{i}}^{a}$} is the arrival or submission time of job i.
\item \textit{${t_{i}}^{s}$} is the start time of job i.
\item \textit{${t_{i}}^{e}$} is the end time of job i.
\item \textit{$w_{i}$} is the width (number of requested / used resources) of job i.
\end{itemize}
From these parameters are computed:
\begin{itemize}
\item \textit{$l_{i}\ =\ {t_{i}}^{e}\ -\ {t_{i}}^{s}$} is the runtime(length) or duration of job i.
\item \textit{${t_{i}}^{w}\ =\ {t_{i}}^{s}\ -\ {t_{i}}^{a}$} is the waiting time of job i.
\item \textit{${t_{i}}^{r}\ =\ {t_{i}}^{w}\ +\ l_{i}$} is the response time of job i.
\item \textit{$s_{i}\ =\ \frac{{t_{i}}^{r}}{l_{i}}\ = 1\ +\ \frac{{t_{i}}^{w}}{l_{i}}$} is the slowdown of job i.
\item \textit{$a_{i}\ =\ w_{i}.l_{i}$} is the area of job i.
\end{itemize}
The performance metrics on which one can evaluate a parallel job scheduler falls into two categories usually with some overlap tolerated between the two:\\ \\
\textbf{User-Centric metrics\cite{streit}}\\
These type of metrics refer to the actual job performance. The performance of the scheduler directly affects the waiting time of jobs. Below mentioned are some of the commonly used user-centric metrics. We will assume $N$ as the number of jobs for which the metric is computed.
\begin{itemize}
\item \textbf{\textit{Average Waiting Time:}} It refers to the average over the waiting time for all jobs.
\boldmath\begin{equation*}
\centering
\text{\textit{AWT}}\ =\ \frac{1}{N}.\sum_{i=1}^{N}{t_{i}}^{w}
\end{equation*}
\item \textbf{\textit{Average Response Time:}} It refers to the time when job results are available. It is computed from the waiting time plus the execution time(time interval from job submission to job completion).
\boldmath\begin{equation*}
\centering
\text{\textit{ART}}\ =\ \frac{1}{N}.\sum_{i=1}^{N}{t_{i}}^{r}
\end{equation*}
\item \textbf{\textit{Average Response Time Weighted by Job Width:}}
\boldmath\begin{equation*}
\centering
\text{\textit{ARTwW}}\ =\ \frac{\sum_{i=1}^{N}{w_{i}.t_{i}}^{r}}{\sum_{i=1}^{N}{w_{i}}}
\end{equation*}
\item \textbf{\textit{Average Slowdown Weighted by Job Area:}}
\boldmath\begin{equation*}
\centering
\text{\textit{SLDwA}}\ =\ \frac{\sum_{i=1}^{N}{a_{i}.s_{i}}}{\sum_{i=1}^{N}{a_{i}}}
\end{equation*}
\end{itemize}
The last 2 metrics would not be suitable to use for adaptive jobs or malleable / evolving jobs since there width changes dynamically at runtime.\\ \\
\textbf{System-Centric metrics\cite{streit}}\\
These type of metrics focus on the effective usage of the resources in the system. Below mentioned are some of the commonly used metrics:
\begin{itemize}
\item \textbf{\textit{Utilization}} Percentage of all resources were actually used on average over a specific time frame
\item \textbf{\textit{Throughput}} Number of jobs that were processed during a specific time frame
\item \textbf{\textit{Makespan}} The time of the completion of the last job in the workload
\item \textbf{\textit{Loss of Capacity}} Percentage of resources that were idle, although workload for processing was available
\end{itemize}
\section{Method of Evaluation}
In order to select a scheduling algorithm for a system, it needs to be evaluated based on certain criterias, many of which have been defined earlier. The performance of such an algorithm depends a lot on the workload which it is processing, hence a workload benchmark maybe useful in evaluating a scheduler or even comparing it with other schedulers. There are various methods used to perform such an evaluation based on a workload\cite{galvin}:
%\subsection{Emulation of Workload}
\begin{itemize}
\item \textbf{\textit{Deterministic Modeling}} This evaluation takes a pre-determined workload and defines the performance of each algorithm for that workload. For example: $5$ jobs arriving at time $0$ in a particular order. One can then use a criteria such as average waiting time to find which algorithm performs the best. This method is simple and fast in giving exact results if the inputs are the same. But, the results only apply to these cases. The main usage of deterministic modeling can be to describe algorithms or serve as examples and over a set of inputs may indicate a trend or behavior that can help for further analysis. 
\item \textbf{\textit{Queueing Models}} The drawback of deterministic modeling is that there is no static set of processes, this varies from day to day. In the method of queueing models, a model of the system is constructed by describing the arrival of jobs having some inter-arrival time, job sizes and job runtime estimates as probability distributions respectively. With all these details, one can then mathematically derive serveral metric values such as average waiting time, utilization, average queue length etc. This method of evaluation is useful in comparing algorithms but has its own limitations. It is only an approximation of the real system and only certain class of algorithms and distributions can be handled as of now in addition to a number of independent assumptions made that may not be accurate.
\item \textbf{\textit{Simulations}} A better and a more accurate approach to evaluating scheduling algorithms is a simulation. In such a method a model of the complete system is implemented using programming and the simulation is driven in time by moving the clock(variable representing time) forward. With time, the state of the system is modified by the simulator to reflect the activities taking place. As the simulation executes, statistics that indicate algorithm performance are gathered and printed. The data to drive the simulation can be generated in several ways: probability distributions for generating job arrivals, job sizes, runtime estimates etc or trace data generated by monitoring real systems and recording the actual sequence of events. Simulations can often be expensive requiring huge hours of computer time, effort to implement the simulator, a more detailed simulator will give a more accurate result but increases the effort.
\item \textbf{\textit{Implementation}} Simulation is of limited accuracy and hence the last approach requires the entire algorithm to be implemented and tested in a real system. This requires much more effort but also brings more uncertainity into the performance of the scheduler as the environment is constantly changing in with real jobs on real physical resources.
\end{itemize}
%\subsection{Real Invasic Applications}
\section{Setup}
The scheduling algorithm implemented in this thesis will be tested by the approach of simulation. This means that the jobs are not actually launched and iRTSched instead will simulate this by transforming the running jobs and modifying the necessary metadata to run the simulation in time. This also means that we do not need actual physical resources or a real cluster to do this testing and just need to emulate a cluster using SLURM to serve our purpose. For the purpose of doing this simulation, below are the steps taken:
\begin{itemize}
\item Configure the SLURM using --enable-front-end and install it. This allows us to emulate a cluster by this front end node which in practical systems will actually dispatch the jobs it receives to all the resources in a cluster managed by a different middleware.
\item Run the SLURM by configuring the scheduler plugin as iBSched and the priority plugin as basic (assigns priorities to jobs in the order of their arrival).
\item Run iRTSched as an independent binary (it is not a SLURM application).
\item iBSched and iRTSched will connect with each other and complete the protocol initialization to prepare for future negotiations.
\item All jobs submitted to SLURM with partition as invasic will be ignored by the main batch scheduler but picked up by the plugin iBSched. This is just a simple hack to redirect the stream of incoming jobs towards iRTSched and not towards the configured front end node. 
\item This has been done to ease the effort for the purpose of simulation. In reality invasic jobs must be directed towards invasic partition and due to some load balancing by the batch system, legacy (rigid) jobs can also be dispatched to the invasic partition when other partitions are busy. 
\item Simulation continues until we shutdown iRTSched and subsequently SLURM. Feedback data from iRTSched will be used to update job states in SLURM. Output data corresponding to job start and end times will be written in a separate file by iBSched for post-simulation analysis.
\end{itemize}
The experiments have been executed on a laptop with a $64$-bit CPU ($8$ cores) each having a max. speed of approx. $3$GHz. The $64$-bit version of Ubuntu $14.04$ LTS is running on the laptop and the version of SLURM used for this thesis is $15.08$.
%%%%%%%
\section{Experiments and Results}
In order to test the prototype, we would need a suitable workload (stream of jobs) for the simulation. There have been prior efforts in this direction to come up with tools, simulators on how to simulate such a system with a workload of different job types\cite{yangjie} or in general to evaluate job schedulers there have been many efforts, some of which are \cite{lucero}\cite{dalibor}. In most of the cases, due to the nature of our architecture which is multi-level and the presence of a negotiation protocol rendered the usage of many of the available simulators as not possible.\\ \\
%%%%%%%%
The \textbf{ESP Benchmark}\cite{wong} was the best choice available to get a synthetic workload that can be suitably modified for simulating a workload with a mix of job types similar to the work of wolf.et.al\cite{laxmikant}. ESP (Effective System Performance) test was devised to provide a metric for resource management on parallel systems that depends upon attributes such as parallel launch time, job scheduling efficiency and job preemption. The objective of the ESP test is to run a fixed number of parallel jobs through a batch scheduler in the minimum elapsed time. Each job  runs the same synthetic application, for a predetermined length of time. The test result is determined, primarily, by the efficiency of the scheduler and the overhead of launching parallel jobs. There are 230 jobs derived from a list of 14 job types. The table below list the job types with their fractional-sizes, instance counts and target run times. Out of the 230 jobs, there are 2 jobs which are of high priority and will occupy the full system. These jobs are not included in the workload as these are of the type "urgent jobs" in our architecture.\\ \\
%%%%%%%%%
The ESP workload has been used to generate 228 jobs for a system with 16 nodes being simulated. The experiments have been conducted by varying the percentage of malleable jobs in the workload along with the remaining rigid jobs. Figures \ref{fig:71} and \ref{fig:72} contain graphs showing the completion time for each of the modified ESP workloads and the average waiting / turnaround times for jobs in the respective workloads.
\begin{figure}[!t]
\centering
%\vspace{-0.5in}
\hspace*{-0.75in}
\includegraphics[width=1.2\textwidth, height=90mm]{./figures/"Workload Completion"}
\caption{Time for completion of the modified ESP workload with varying amounts of rigid and malleable jobs}
\label{fig:71}
\end{figure}
\begin{itemize}
\item All the jobs are assumed to have linear speedup curves for the purpose of simulation. The runtime scheduler will adjust the expected end times of all the running malleable jobs every time they get transformed.
\item Figure \ref{fig:71} shows consistent behavior irrespective of the variation in the percentages of the malleable jobs. This shows predictable quality of service from the schedulers and can be safely used for a workload in a HPC system with predictable or unpredictable variation in the number of malleable jobs at any given point of time.
\item The workload with $10$\% malleable jobs is almost as good as a rigid workload. The opportunity to expand or shrink in such a workload in order to increase the throughput is very less and is possibly the reason for a longer completion time / lower throughput compared to others.
\item Increase in throughput really happens only if the running malleable jobs are expanded in case they can facilitate an earlier start time of the batch jobs according to the DBES algorithm but not only for the reasons that the percentage of malleable jobs is high.
\item The utilization levels were almost close to a $100\%$ for all the experiments and is the reason why there have been no graphs shown for the same. High utilization levels do not always translate to a higher throughput but the reason could be due to the presence of malleable jobs. Only, if they are expanded or shrunk intelligently can both the throughput and utilization be kept high. DBES plays its role in achieving the same.
\item Higher utilization levels are also due to the fact that there is a negotiation happening and this provides a very dynamic and flexible scheduling strategy. Negotiation finds a common point where both batch and runtime schedulers are happy and this usually can find a better packing of batch jobs by going through different transformations of running jobs. 
\item This is in contrast to the conventional approaches which are rigid and do not have the opportunity to provide alternative schedules for batch jobs with respect to the transformations of the running jobs as seen in the original DBES algorithm.
\item Figure \ref{fig:72} also shows fairly consistent average completion and response times of the jobs in different workloads. This shows that variations in the number of malleable jobs within the workload does not have an adverse impact on these metrics due to the proposed algorithm apart from small variations.
\end{itemize}
%%%%%%%%%%
\begin{figure}[!htbp]
\centering
%\vspace{-0.5in}
\hspace*{-0.75in}
\includegraphics[width=1.2\textwidth, height=85mm]{./figures/"avg-compl-resp".pdf}
\caption{Average Waiting Time and Average Response Time for the Modified ESP workload}
\label{fig:72}
\end{figure}
%%%%%%%
\begin{itemize}
\item Figure \ref{fig:73} shows a graph with few selected workloads of $10\%$, $50\%$ and $100\%$ malleable jobs for simulating the system without any expand or shrink operations. This means that these malleable jobs are essentially moldable jobs now since they can be assigned any size within their range but only at submission time. Remaining jobs are as before of rigid type.
\item All the three workloads take around $240$ min to finish. This is only slightly higher than the malleable-rigid workload that stood around $230$ min. This tells us that the proposed negotiation based flexible DBES algorithm has definitely provided us with a possibly good performance for moldable-rigid workload in comparison to the malleable-rigid workload.
\item The results for $10\%$ of malleable jobs in the workload seems to be an anomaly in the experiments conducted. This could probably indicate to us that the performance of the algorithms are poor for this specific case in all the categories of experiments. This is quite similar to the case with the original DBES algorithm. 
\item The previous scenario would be an important area of improvement in the future work of this research problem.
\item Looking the other way around, there is a greater scope to achieve better results for malleable-rigid workload by performance-based expand or shrink operations and a more intelligent negotiation protocol. Batch scheduling algorithm can also make better decisions by considering feedback from runtime scheduler.
\end{itemize}
%%%%%%%%%%
\begin{figure}[!htbp]
\centering
%\vspace{-0.5in}
\hspace*{-0.75in}
\includegraphics[width=1.2\textwidth, height=85mm]{./figures/"moldable+rigid".pdf}
\caption{Time for completion of the modified ESP workload with varying amounts of rigid and moldable jobs}
\label{fig:73}
\end{figure}
%%%%%%%%
