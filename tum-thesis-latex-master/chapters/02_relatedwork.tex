\chapter{Related Work}\label{chapter:related work}
The problem of job scheduling for adaptive applications as introduced in the previous chapter is really at the intersection of many different problems. In this section, we will look at the some of the relevant work that has happened in the past with respect to batch job scheduling and also briefly look at a subset of the relevant work that has happened with resource-aware programming models and runtime scheduling.\\ \\
%%%%%%%%%%
Substantial amount of work has been done in exploring efficient resource management and scheduling for HPC systems in the past focusing on rigid and moldable jobs. There has been growing interest in the recent years to explore dynamic resource management and scheduling techniques to address the challenges of future exascale machines and the complex applications that will try to exploit this massive parallelism. The complexity of applications are increasing, ex: Scientific applications utilizing \textbf{\textit{Adapative Mesh Refinement(AMR)}} techniques change their working set size as the mesh is refined/coarsened. Dynamically allocating resources to such applications at runtime or other applications which are potentially scalable has the potential to improve resource utilization, energy efficiency, fault tolerance\cite{vadhiyar} and a host of other metrics\cite{abhishek}\cite{osman}. Wolf.et.al\cite{felix} demonstrated such a dynamic resource management approach for supporting evolving jobs by extending the Torque / Maui Batch System to allow dynamic allocations and a dynamic fairness strategy implemented in the Maui Scheduler to efficiently service static and dynamic allocations. Their results showed reduced turnaround and waiting times for applications, while increasing system utilization and throughput. Wolf.et.al\cite{laxmikant} also proposed an extension to this work to further support malleable type of jobs with the help of a communication protocol between the Batch System and the Charm++ runtime which enables the malleability of applications. In combination with their earlier work, The Scheduler now supported a mix of jobs with different types such as rigid, malleable and evolving by using an intelligent algorithm called DBES (Dependency-based expand shrink). Several other efforts have been made earlier in order to support scheduling such a mix of job types\cite{hungershofer}\cite{jamjoom}\\ \\
%%%%%%%%%%%
In contrast to the research efforts mentioned above that focused on the Charm++, there have been several efforts directed towards MPI\cite{georgiou}\cite{travis}\cite{gladys}\cite{gonzalo}\cite{martin}\cite{gonzalo} some of which explored on how to first provide such an adaptive behavior on MPI applications and some on how to support the same with the help of libraries, resource management systems\cite{klein} etc. In the current collaborative research project earlier efforts were directed towards supporting such adaptive programming paradigms in shared memory programming model such as openMP\cite{andreas} and work was also done in MPI\cite{isaias}. The ongoing efforts in this project is to now continue forward in the direction of MPI but to approach the problem vertically at different levels of abstraction such as the programming model, middleware and runtime tuning of applications.\\ \\
%%%%%%%%%
This thesis will mainly focus on the middleware part of this solution stack to support adaptive applications in future HPC systems. Middlewares provide job scheduling for HPC systems and job scheduling has always remained a very active area of research both for theoretical purposes and for practical systems. The most widely used and proven technique running in most of the HPC systems today is \textbf{\textit{backfilling}}. Backfilling was first developed by lifka\cite{david} for the Argonne National Laboratory's IBM SP system to address the need for a new scheduling system on supercomputers and was named as \textbf{EASY}(Extensible Argonne Scheduling System). Feitelson.et.al\cite{ahuva} proposed an extension to backfilling by providing a reservation for every job that could not be started and allowing lower priority jobs from behind in the queue to start if they would not delay these reservations. This was called as \textbf{\textit{Conservative Backfilling}} whereas the one proposed by lifka was called \textbf{\textit{Aggressive Backfilling}} as it provided a reservation to the job only at the front of the queue. Feitelson.et.al\cite{dror} came up with a new approach to backfilling algorithm where a set of jobs from the job queue are looked at once for making a scheduling decision using dynamic programming instead of traversing the queue one job at a time. The claim was that this resulted in better packing of jobs resulting in higher utilization, reduced mean response time and mean slowdown of all jobs. This scheduler was named as \textbf{\textit{LOS(Lookahead Optimizing Scheduler)}}. Efforts have also been made for having intelligent batch scheduling algorithms towards better IO utilization, energy efficiency and several other metrics\cite{zhou}\cite{desai}\cite{yang}\cite{daniel}\cite{dongxu} \\ \\
%%%%%%%%%%
In addition to the standard batch job scheduling for rigid jobs, efforts have also been made in the direction of supporting moldable jobs. Cirne.et.al\cite{wcirne} proposed \textbf{SA(Supercomputer AppLes - Application level scheduler)} that would select on behalf of the user, the appropriate size for a given job from its available sizes. This decision was made based on the state of the resources, job characteristics etc to minimize turnaround time. Sadayappan.et.al\cite{sabin}\cite{srividya}\cite{sudha} proposed several works for moldable scheduling of parallel jobs by using different policies like fair-share, overbooking, and finally considering job's efficiency also in determining the best partition size for the job. They developed an iterative algorithm in order to make the appropriate choice of values for overbooking and efficiency which is inturn based on the scalability characteristics of the job mix.\\ \\
%%%%%%%%
Several efforts have also been made in the direction of scheduling malleable jobs where frameworks have been developed\cite{wcirne1} in order to handle the runtime scheduling of such type of jobs. These frameworks take into consideration several policies and performance / scalability criteria of running applications into its decision making. Deshmeh\cite{deshmeh} proposed a performance prediction tool called \textbf{ADEPT (Automatic Downey-Based Envelope-Constrained Prediction Tool)} that will predict with high accuracy the performance of the application while requiring only few observations. Sudarshan.et.al\cite{sudarshan}\cite{rajesh} proposed a framework that provided support for dynamically resizing the applications. It provided data redistribution algorithms required while reisizing the applications, performance-sensitive scheduling of such applications with the help of smart policies and strategies.\\ \\
%%%%%%%%
Traditionally, Batch Systems have had both batch and runtime scheduling merged together into a single component. In this work, we explore the idea of separating the two concerns into different components for providing a dynamic and flexible scheduling functionality. Such a multi-level approach has usually been observed in the grid and cloud based infrastructures\cite{kwang}\cite{kurowski}\cite{streit}\cite{streit1}. These infrastructures are distributed geographically and employ a local resource manager and scheduler at each of these locations and all of these locations are coordinating by a single centralized broker / service provider. The broker usually engages in some sort of a negotiation\cite{oliver}\cite{roland}\cite{viktor}\cite{rizos} with the different locations / resource providers / sites for availability of resources in order to make the best possible decision for serving its users.
